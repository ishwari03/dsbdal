{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70541b16-2f23-4ab9-9361-00fe91c51749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pusad\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\pusad\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pusad\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pusad\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pusad\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pusad\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a0b324-696a-48bb-ba93-cb91ad78fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dacc486-d4ef-4bb0-bfe5-7659029392bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pusad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pusad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pusad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\pusad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pusad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6c4cc9-2e06-471f-8c09-1f1942b7b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5be90fb-b4fe-4f07-9b4f-55f2a31eda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello Everyone! I am Ishwari Pusadkar pursuing Computer Engineering in MES College Of Engineering. I am currently in Third Year Of Computer Engineering.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68dbdcf-a5a8-43e6-9b10-5174360c3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      " ['Hello Everyone!', 'I am Ishwari Pusadkar pursuing Computer Engineering in MES College Of Engineering.', 'I am currently in Third Year Of Computer Engineering.']\n"
     ]
    }
   ],
   "source": [
    "# 1. Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\\n\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae57e1c8-0e17-4b68-b0e1-cf6c169dbbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      " ['Hello', 'Everyone', '!', 'I', 'am', 'Ishwari', 'Pusadkar', 'pursuing', 'Computer', 'Engineering', 'in', 'MES', 'College', 'Of', 'Engineering', '.', 'I', 'am', 'currently', 'in', 'Third', 'Year', 'Of', 'Computer', 'Engineering', '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d71b7740-43d9-4d7f-bd95-a11463105e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:\n",
      " [('Hello', 'NNP'), ('Everyone', 'NN'), ('!', '.'), ('I', 'PRP'), ('am', 'VBP'), ('Ishwari', 'NNP'), ('Pusadkar', 'NNP'), ('pursuing', 'VBG'), ('Computer', 'NNP'), ('Engineering', 'NNP'), ('in', 'IN'), ('MES', 'NNP'), ('College', 'NNP'), ('Of', 'IN'), ('Engineering', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('in', 'IN'), ('Third', 'NNP'), ('Year', 'NNP'), ('Of', 'IN'), ('Computer', 'NNP'), ('Engineering', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 3. POS Tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(\"POS Tagging:\\n\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3f5901b-a38e-46b9-94d2-fdb4ab6e1948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Sentence: \n",
      " ['hello', 'everyone', 'i', 'am', 'ishwari', 'pusadkar', 'pursuing', 'computer', 'engineering', 'in', 'mes', 'college', 'of', 'engineering', 'i', 'am', 'currently', 'in', 'third', 'year', 'of', 'computer', 'engineering']\n",
      "\n",
      "Filtered Words (no stopwords & no punctuation):\n",
      " ['hello', 'everyone', 'ishwari', 'pusadkar', 'pursuing', 'computer', 'engineering', 'mes', 'college', 'engineering', 'currently', 'third', 'year', 'computer', 'engineering']\n"
     ]
    }
   ],
   "source": [
    "# 4. Removing Punctuation and Stopwords\n",
    "text_clean = re.sub('[^a-zA-Z]', ' ', text)\n",
    "tokens = word_tokenize(text_clean.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "print(\"\\nTokenized Sentence: \\n\", tokens)\n",
    "print(\"\\nFiltered Words (no stopwords & no punctuation):\\n\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77284ca1-4cbb-47fd-9db1-d979c57b6a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write\n"
     ]
    }
   ],
   "source": [
    "# 5. Stemming\n",
    "e_words = [\"write\",\"writing\",\"wrote\",\"writes\"]\n",
    "ps = PorterStemmer()\n",
    "for word in e_words:\n",
    "  rootWord=ps.stem(word)\n",
    "print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5da2258b-34bb-4b40-8b3d-15a99e24cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies studying cries cry\n",
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "# 6.Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tt = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "for word in tt:\n",
    "    print(\"Lemma for {} is {}\".format(word, wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bb33b52-4bac-4b88-a2b9-d7e585071ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "      about        am       and        as        at  bachelors  bootstrap  \\\n",
      "0  0.000000  0.423394  0.000000  0.000000  0.211697   0.211697   0.000000   \n",
      "1  0.288675  0.000000  0.288675  0.000000  0.000000   0.000000   0.288675   \n",
      "2  0.000000  0.000000  0.000000  0.271379  0.000000   0.000000   0.000000   \n",
      "\n",
      "    college  computer       css  ...        my        of  pursuing  pusadkar  \\\n",
      "0  0.211697  0.211697  0.000000  ...  0.000000  0.211697  0.211697  0.211697   \n",
      "1  0.000000  0.000000  0.288675  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.271379  0.000000  0.000000  0.000000   \n",
      "\n",
      "    relaxed   science      time        to        ty     wadia  \n",
      "0  0.000000  0.211697  0.000000  0.000000  0.211697  0.211697  \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.271379  0.000000  0.271379  0.271379  0.000000  0.000000  \n",
      "\n",
      "[3 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "#2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample Documents\n",
    "documents = [\n",
    "    \"Hello everyone! I am Ishwari Pusadkar. I am in TY pursuing Bachelors Degree in Computer Science at MES Wadia College Of Engineering.\",\n",
    "    \"I have good knowledge about C, C++, Java, Jython, HTML, CSS, JS, Bootstrap and Databases.\",\n",
    "    \"In my free time, I like listening to music as it makes me feel relaxed.\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the documents to get the TF-IDF matrix\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DataFrame\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e08072-768d-4c52-944c-0fc74821c44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
